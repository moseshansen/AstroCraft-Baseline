import csv
import imageio
import math
import random
import matplotlib
matplotlib.use("TkAgg")
import matplotlib.pyplot as plt
from collections import namedtuple, deque
import torch
import torch.nn as nn
from PettingZoo_MA.env.CaptureTheFlagMA import CTFENVMA
import numpy as np #all the imports
import numpy as np
from PettingZoo_MA.heuristic_action_select import DormantOpponent #give_action
#from PettingZoo_MA.heuristic_action_select import give_action
from PettingZoo_MA.env.constants import MAX_FUEL, PTACT, PCUBE, PTCOL, PTMAX, PRAD, R0, DH, DHACTUAL, MIN_FUEL_DICT, SLOW_FUEL_COSTS, FAST_FUEL_COSTS
import seaborn as sns
sns.set()
import pickle
histogram_list = []

n_agents = 1
generator = DormantOpponent(n_agents)


env1v1 = CTFENVMA(team_size=1, win_rew=1, flag_rew=0)
obs3v3, info3v3 = env1v1.reset() #initializes the environment 

is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display

plt.ion()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") #chooses gpu, if no gpu then chooses cpu
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ReplayMemory(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        """Save a transition"""
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class MultiAgentNN(nn.Module): #for 3 agents, multiplies the first and last part by the number of agent
    def __init__(self, n_observations, hidden_dim, output_dim, num_agents):
        super(MultiAgentNN, self).__init__()
        self.output_dim = output_dim
        self.num_agents = num_agents
        self.fc1 = nn.Linear(n_observations *num_agents, hidden_dim) #input (24 for each agent) *3 agents = 72 (already 72 multiplied beforeee)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim * num_agents)  #number of actions times number of agents

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x).view(-1, self.num_agents, self.output_dim)  # Ensure correct reshaping
    
hidden_dim = 128 #any number
num_agents = 1 #since it's 3v3 
BATCH_SIZE = 32 #any batch size 
GAMMA = 0.99 
EPS_START = .95
EPS_END = 0.1
EPS_DECAY = 100000
TAU = 0.03
LR = 1e-4


n_actions = 14 #actions available to an agent
n_observations = 36 # Total observations for one agent
policy_net = MultiAgentNN(n_observations, hidden_dim, n_actions, num_agents).to(device)
#policy_net.load_state_dict(torch.load('baseline_results/curriculum_learning_3pi6.pth', map_location=device))
target_net = MultiAgentNN(n_observations, hidden_dim, n_actions, num_agents).to(device)
policy_dict = policy_net.state_dict()
target_dict = target_net.state_dict()
# for key in policy_dict:
#     if key not in target_dict:
#         print(f"Key {key} not found in target network.")
#     elif policy_dict[key].shape != target_dict[key].shape:
#         print(f"Shape mismatch for {key}: {policy_dict[key].shape} vs {target_dict[key].shape}")
target_net.load_state_dict(policy_net.state_dict())
optimizer = torch.optim.Adam(policy_net.parameters(), lr=LR)
memory = ReplayMemory(100000)
steps_done = 0

def select_action2(state, mask):
    state_tensor = torch.from_numpy(state.flatten()).unsqueeze(0).to(device)
    q_values = target_net(state_tensor)
    actions = []
    for i in range(1):
        mask_tensor = torch.from_numpy(mask[i]).bool().to(device)
        q_values_masked = q_values[0][ i].masked_fill(~mask_tensor, float('-inf')) #fills it in if it's in the mask, otherwise no 
        actions.append(torch.argmax(q_values_masked).item())
    print("action1", end= "\r")
    return np.array(actions)

def select_action(state, mask):
    global steps_done
    sample = random.random()  # Random number for epsilon-greedy policy
    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    #eps_threshold = 1 #change later!!!
    state_tensor = torch.from_numpy(state.flatten()).unsqueeze(0).to(device)
    action = []
    if sample > eps_threshold:
        q_values = target_net(state_tensor)
        actions = []
        for i in range(1):
            mask_tensor = torch.from_numpy(mask[i]).bool().to(device)
            q_values_masked = q_values[0][ i].masked_fill(~mask_tensor, float('-inf')) #fills it in if it's in the mask, otherwise no 
            actions.append(torch.argmax(q_values_masked).item())
        print("action1", end= "\r")
        return np.array(actions)
    else:
        action0 = env1v1.action_space('player0').sample(obs3v3['player0']['action_mask'])  #random
        print("action2", end= "\r")
        return action0
episode_durations = []
episode_rewards = []
def plot_rewards(show_result=False): #just to plot :))) 
    plt.figure(1)
    rewards = torch.tensor(episode_rewards, dtype=torch.float)
    if show_result:
        plt.title('Result')
    else:
        plt.clf()
        plt.title('Training...')
    plt.xlabel('Episode')
    plt.ylabel('Episodic RANDOM Agent Reward')
    plt.plot(rewards.numpy())
    # Take 100 episode averages and plot them too
    if len(rewards) >= 5:
        means = rewards.unfold(0, 100, 1).mean(1).view(-1)
        means = torch.cat((torch.zeros(99), means))
        plt.plot(means.numpy())

    plt.pause(0.001)  # pause a bit so that plots are updated
    if is_ipython:
        if not show_result:
            display.display(plt.gcf())
            display.clear_output(wait=True)
        else:
            display.display(plt.gcf())

def plot_durations(show_result=False):
    plt.figure(2)
    durations_t = torch.tensor(episode_durations, dtype=torch.float)
    if show_result:
        plt.title('Result')
    else:
        plt.clf()
        plt.title('Training...')
    plt.xlabel('Episode')
    plt.ylabel('Duration RANDOM Agent')
    plt.plot(durations_t.numpy())
    # Take 100 episode averages and plot them too
    if len(durations_t) >= 5:
        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
        means = torch.cat((torch.zeros(99), means))
        plt.plot(means.numpy())
    plt.pause(0.001)  # pause a bit so that plots are updated
    if is_ipython:
        if not show_result:
            display.display(plt.gcf())
            display.clear_output(wait=True)
        else:
            display.display(plt.gcf())

def append_value_to_csv(file_name, value):
    with open(file_name, mode='a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow([value])  # Write a single value as a new row

def optimize_model():
    double= True
    if len(memory) < BATCH_SIZE:
        return
    transitions = memory.sample(BATCH_SIZE)
    batch = Transition(*zip(*transitions))
    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)
    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).view(BATCH_SIZE, -1).to(device) #reshapes to be the same size as the state one 

    state_batch = torch.cat(batch.state).view(BATCH_SIZE, -1).to(device)
    action_batch = torch.cat(batch.action).view(BATCH_SIZE,1, 1).to(device)
    reward_batch = torch.cat(batch.reward).to(device)
    state_action_values = policy_net(state_batch).gather(2, action_batch)
    next_state_values = torch.zeros(BATCH_SIZE, 1,device=device)
    # if double:
    #     with torch.no_grad():
    #         next_state_actions = policy_net(non_final_next_states).max(2)[1].unsqueeze(1)
    #         next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_state_actions).squeeze().detach() #uses target net also 
    if double:
        with torch.no_grad():
            next_state_actions = policy_net(non_final_next_states).max(2)[1].unsqueeze(2)  # Ensure dimensions match
            target_net_output = target_net(non_final_next_states)
            next_state_values[non_final_mask] = target_net_output.gather(2, next_state_actions).squeeze(2).detach()
    
    else:
        with torch.no_grad():
            next_state_values[non_final_mask] = target_net(non_final_next_states).max(2)[0].detach() #gets an array (batch_size, 3)

    #expected_state_action_values = ((next_state_values ) - reward_batch.view(BATCH_SIZE, 1))*GAMMA
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch.view(BATCH_SIZE, 1)
    #criterion = nn.SmoothL1Loss()
    loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(2))
    optimizer.zero_grad()
    loss.backward()
    for param in policy_net.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()

num_episodes = 1000
basemax = []
fuel_amount = []
flagged_max = []
return_max = []
#values2 = []
for _ in range(1000):
    basemax.append([])
    fuel_amount.append([])
    flagged_max.append([])
    return_max.append([])

rew_buffer = deque([0, 0], maxlen=10)
n_agents = 1
trajectories = []
current_trajectory = []
trajectory_counter = 0

for i_episode in range(num_episodes):
    
    #flagged = False
    #find orbit angle?? 
    step = 0
    if i_episode%2==0:
        print(i_episode, "completed!")

        
    state, info = env1v1.reset() #resets the environment for each episode
    t=0
    n_agents = 1
    episode_reward = 0
    obs3v3, info3v3 = env1v1.reset()
   

    basemax[i_episode]=0
    flagged_max[i_episode] = 0
    return_max[i_episode] = 0

    done = False


    while not done:
        #print('state', state)
        if(obs3v3['player0']['observation'][1][7]-obs3v3['player0']['observation'][0][7]>basemax[i_episode]):
            basemax[i_episode] = obs3v3['player0']['observation'][1][7]-obs3v3['player0']['observation'][0][7]
        has_flag_1 = [n for n in range(n_agents+1) if env1v1._player0[n]._flagged and not env1v1._player1[n]._transferring]
        if(has_flag_1):
            flagged_max[i_episode] = 1
        
        #if(step==10):
            #print(state)
        mask1 = obs3v3['player0']['action_mask']
        state = obs3v3['player0']['observation']
        state_modified = torch.tensor(obs3v3['player0']['observation'], dtype=torch.float32, device=device) #makes the state the correct shape
        action0 = env1v1.action_space('player0').sample(obs3v3['player0']['action_mask'])  #random
        #action0 = select_action(state ,mask1) #selects the actual action for the agent 
        #print("action selected:", action0)

        mask2 = np.array(obs3v3['player1']['action_mask'])  # Ensure this is a 2D array
        state2 = np.array(obs3v3['player1']['observation'])  # Ensure this is a 2D array
        action1 = generator.select_action(obs3v3['player1'])
        #action1 = env1v1.action_space('player1').sample(obs3v3['player1']['action_mask'])  #random

        action1= torch.tensor(action1, device=device, dtype=torch.long) #find the action taken
        action = { 'player0': action0,'player1': action1}

        obs3v3, reward, term, trunc, info = env1v1.step(action)

        current_trajectory.append([obs3v3, reward, term, trunc, info, action, mask1])
        episode_reward+= reward['player0']

        done = term['player0'] or term['player1'] or (trunc['player0'] or trunc['player1'])
        if not done:
            step+=1
            next_state = torch.tensor(obs3v3['player0']['observation'], dtype=torch.float32, device=device) #makes the observation the next state
            action0= torch.tensor(action0, device=device, dtype=torch.long) #find the action taken
            
            reward_1 = torch.tensor([reward['player0']], dtype=torch.float) #records the reward 3 times 
            memory.push(state_modified, action0, next_state, reward_1)
            state2 = state_modified #makes state 2 that updates some things
            state = next_state

        optimize_model()
        target_net_state_dict = target_net.state_dict()
        policy_net_state_dict = policy_net.state_dict()
        for key in policy_net_state_dict:
            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)
        target_net.load_state_dict(target_net_state_dict)
        if done:
            fuel_amount[i_episode] = 1000 - state[1][3]
            if(episode_reward>=1):
                return_max[i_episode]= env1v1.find_who_ran_out(['player0'])
            trajectories.append(current_trajectory)
            current_trajectory = []
            trajectory_counter += 1
            if (trajectory_counter % 100) == 0:
                print("DUMPING FILE: ", int(trajectory_counter/100 - 1))
                with open('baseline_results/rllearning' +
                          str(int(trajectory_counter/100 - 1)) +
                          '.pkl', 'wb') as f:
                    pickle.dump(trajectories, f)
                trajectories = []

            print("episode reward:",episode_reward, "episode_duration", step+1 )
            episode_durations.append(step+1)
            episode_rewards.append(episode_reward)
            episode_reward = 0
            break
        print("reward:",reward_1 , "step:", step, end= "\r")

eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)
print('eps_threshhold', eps_threshold)
np.save('baseline_results/episode_durationspi6.npy', basemax)
np.save('baseline_results/episode_durationspi6.npy', fuel_amount)
np.save('baseline_results/episode_durationspi6.npy', flagged_max)
np.save('baseline_results/episode_durationspi6.npy', return_max)
print("basemax", basemax)
print("basemax", fuel_amount)
print("basemax", flagged_max)
print("return max", return_max)
print("mean basemax:", np.mean(basemax))
print("mean fuel used:", np.mean(fuel_amount))
print("mean flagged:", np.mean(flagged_max))
print("mean return max:", np.mean(return_max))





